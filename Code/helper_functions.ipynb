{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-plot\n",
      "  Using cached https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from scikit-plot) (3.0.3)\n",
      "Collecting joblib>=0.10 (from scikit-plot)\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from scikit-plot) (0.20.2)\n",
      "Requirement already satisfied: scipy>=0.9 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from scikit-plot) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.16.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.0)\n",
      "Requirement already satisfied: six in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (40.8.0)\n",
      "Installing collected packages: joblib, scikit-plot\n",
      "Successfully installed joblib-0.13.2 scikit-plot-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached https://files.pythonhosted.org/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: scipy in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from xgboost) (1.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from xgboost) (1.16.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.90\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/e5/4c/7557e1c2e791bd43878f8c82065bddc5798252084f26ef44527c02262af1/imbalanced_learn-0.4.3-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from imbalanced-learn) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from imbalanced-learn) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from imbalanced-learn) (0.20.2)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (4.32.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /u1/sjameled/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /u1/sjameled/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /u1/sjameled/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from sklearn import preprocessing, pipeline, feature_extraction, decomposition, model_selection, metrics, cross_validation, svm\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scikitplot.plotters as skplt\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from collections import Counter\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from keras) (1.2.1)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting h5py (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from keras) (1.16.2)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in /usr/local/Anaconda3-5.3.1-Linux-x86_64/envs/jupyter-atp/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Installing collected packages: h5py, keras-applications, keras-preprocessing, keras\n",
      "Successfully installed h5py-2.9.0 keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,Conv2D, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras.optimizers import *\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_classes(df):\n",
    "    '''\n",
    "    this function will adjust the target format\n",
    "    input: \n",
    "    df: dataframe with classes\n",
    "    output:\n",
    "    df: dataframe with just one col for the class\n",
    "    '''\n",
    "    df['Class']=df['class1']+2*df['class2']+3*df['class3']\\\n",
    "                        +4*df['class4']+5*df['class5']+6*df['class6']\\\n",
    "                        +7*df['class7']+8*df['class8']+9*df['class9']\n",
    "    df.drop(['class1','class2','class3','class4','class5','class6','class7','class8','class9'],axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_freq(df):\n",
    "    '''\n",
    "    this function calculates the percentage of each class \n",
    "    input\n",
    "    df: dataframe\n",
    "    output\n",
    "    a plot with the class frequency \n",
    "    '''\n",
    "    class_freq=100*df.groupby('Class')['ID'].count()/df['ID'].count()\n",
    "    plt.title('Class Frequecy in percentage')\n",
    "    class_freq.sort_values(ascending=False).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_class(df):\n",
    "    '''\n",
    "    this function calculates the number of genes in each class\n",
    "    input\n",
    "    df: dataframe with gene and class\n",
    "    output\n",
    "    gene_class_freq: table with the number of gene/class pair\n",
    "    '''\n",
    "    gene_class_freq=df_train.groupby(['Gene','Class'])['ID'].count().unstack().fillna(0).apply(lambda x:round((x/np.sum(x)),2),axis=1)\n",
    "    return gene_class_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_class_unique(df):\n",
    "    '''\n",
    "    this function calculates the number of class for each gene\n",
    "    input: df \n",
    "    output\n",
    "    unique_class: number of class per gene\n",
    "    '''\n",
    "    unique_class=df.groupby(['Gene'])['Class'].nunique()\n",
    "    return unique_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_basic = re.compile(r'^([a-zA-Z\\*])(\\d{1,7})([a-zA-Z\\*])$',)\n",
    "truncations = ['truncating mutations','trunc']\n",
    "deletion_insertion = ['insertions/deletions','deletion/insertion','delins']\n",
    "fusion = ['fusions','fusion','fus','fs*','fs']\n",
    "deletion = ['deletions','deletion','del']\n",
    "insertion = ['insertions','insertion','ins']\n",
    "amplification = ['amplification']\n",
    "overexpression= ['overexpression']\n",
    "splice= ['splice']\n",
    "duplication= ['duplications','duplication','dup']\n",
    "all_variation=[truncations,deletion_insertion,fusion,deletion,insertion,amplification,overexpression,splice,duplication]\n",
    "all_variation_names=['truncation','deletion_insertion','fusion','deletion','insertion','amplification','overexpression','splice','duplication']\n",
    "\n",
    "def family_var(variation):\n",
    "    '''\n",
    "    this function will perform feature engineering on the variation column\n",
    "    it will extract the operation\n",
    "    extract first,last letter if the operation was a substitution, the O notation is for original and S for switched\n",
    "    input: variation\n",
    "    output: operation, first last letter and location\n",
    "    '''\n",
    "    family = {'O_letter':'none','location':0,'S_letter':'none','operation':'other'}\n",
    "    var_words=variation.split(' ')\n",
    "    i=len(var_words)\n",
    "    m=re.match(rule_basic,var_words[0])\n",
    "    if i==1 and m:\n",
    "        family['operation']='substitution'\n",
    "        if m.group(1)!='*':\n",
    "            family['O_letter']=m.group(1)\n",
    "        family['location']=m.group(2)\n",
    "        if m.group(3)!='*':\n",
    "            family['S_letter']=m.group(3)\n",
    "    else:\n",
    "        flag=False\n",
    "        txt=variation.lower()\n",
    "        for index,item in enumerate(all_variation):\n",
    "            for x in item:\n",
    "                if x in txt:\n",
    "                    flag=True\n",
    "                    family['operation']=all_variation_names[index]\n",
    "                    txt=''\n",
    "                    break\n",
    "        \n",
    "        \n",
    "    return family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_gene(variation):\n",
    "    '''\n",
    "    this function will add the second gene if the variant operation is fusion and the format shows the presence of that 2nd gene\n",
    "    input: variation\n",
    "    output: gene2\n",
    "    '''\n",
    "    variation=variation.lower()\n",
    "    pos=variation.find('-')\n",
    "    pos_fusion=variation.find('fusion')\n",
    "    gene2=''\n",
    "    if pos!=-1 and pos_fusion!=-1:\n",
    "        gene2=variation[pos+1:pos_fusion-1]\n",
    "    return gene2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df):\n",
    "    '''\n",
    "    this function will apply the above functions to do the cleaning on the data frame\n",
    "    the function will also hot encore the categorical variables with get dummies\n",
    "    input: df\n",
    "    output: cleaned df with additional columns and hot encoded variables\n",
    "    '''\n",
    "    df_variations=pd.DataFrame(list(df['Variation'].apply(family_var)))\n",
    "    df=pd.concat([df,df_variations],axis=1)\n",
    "    df['second_gene']=df['Variation'].apply(second_gene)\n",
    "    df=pd.get_dummies(df,columns=['operation','Gene', 'Variation','O_letter', 'S_letter','second_gene'],drop_first=True)\n",
    "    #lbl = preprocessing.LabelEncoder()\n",
    "    #for col in['operation','Gene', 'Variation','O_letter', 'S_letter','second_gene']:\n",
    "        #df[col]=lbl.fit_transform(df[col].values)\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words = [\"fig\", \"figure\", \"et\", \"al\", \"al.\", \"also\",\n",
    "                \"data\", \"analyze\", \"study\", \"table\", \"using\",\n",
    "                \"method\", \"result\", \"conclusion\", \"author\",\"patient\",\"analysis\",\"expression\",\n",
    "                \"find\", \"found\", \"show\", '\"', \"’\", \"“\", \"”\"]\n",
    "stop_words = set(stopwords.words('english') + list(punctuation) + custom_words)\n",
    "def nlp_processing(text):\n",
    "    '''\n",
    "    this function will perform text cleaning by removing special characters, lowering caps and lemmatizing\n",
    "    input: text\n",
    "    output: string hich is the cleaned version of the text\n",
    "    '''\n",
    "    string=''\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9-]\", \" \", str(text))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text=text.lower()\n",
    "    \n",
    "    \n",
    "    for word in text.split(' '):\n",
    "        if (word not in stop_words) and len(word)>2:\n",
    "            clean_word=lemmatizer.lemmatize(word)\n",
    "            string+=clean_word+' '\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these classes will build new transform fit methods to feed in our pipelines\n",
    "class cust_regression_vals(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        x = x.drop(['clean_text'],axis=1).values\n",
    "        return x\n",
    "\n",
    "class cust_txt_col(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        return x[self.key].apply(str)\n",
    "fp = pipeline.Pipeline([\n",
    "    ('union', pipeline.FeatureUnion(\n",
    "        n_jobs = -1,\n",
    "        transformer_list = [\n",
    "            ('text', pipeline.Pipeline([('clean_text', cust_txt_col('clean_text')),  \n",
    "                                        ('tfidf', TfidfVectorizer()),\n",
    "                                        ('truncatedsvd',TruncatedSVD(n_components=500, n_iter=100, random_state=42))\n",
    "                                    ])),\n",
    "             ('others', Pipeline([\n",
    "                                 ('standard', cust_regression_vals()),\n",
    "                                 ('wscaler', StandardScaler()),\n",
    "             ]))\n",
    "        ])\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(x1,y1,x2,y2, clf=None, ):\n",
    "    '''\n",
    "    this function will fit and train a classifier \n",
    "    input: train, test data (features and target), classifier\n",
    "    output: performance metrics log loss, accuracy, classification report and confusion matrix\n",
    "    '''\n",
    "    if clf is None:\n",
    "        clf = RandomForestClassifier()\n",
    "    clf.fit(x1,y1)\n",
    "    probas = cross_val_predict(clf, x2,y2, n_jobs=-1, method='predict_proba', verbose=2)\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.unique(y2)\n",
    "    preds = classes[pred_indices]\n",
    "    print('Log loss: {}'.format(log_loss(y2, probas)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y2, preds)))\n",
    "    print('Classification report: {}'.format(classification_report(y2,preds)))\n",
    "    skplt.plot_confusion_matrix(y2, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc2vec(model,train_tagged,test_tagged):\n",
    "    '''\n",
    "    this function will perform a doc2vec model\n",
    "    input: tagged train and test data\n",
    "    output: transformed train and test via doc2vec model\n",
    "    '''\n",
    "    model.build_vocab(train_tagged,update = True)\n",
    "    model.train(train_tagged,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    train_vecs_dm = []\n",
    "    for doc in train_tagged:\n",
    "        vec = d2v_model_dm.infer_vector(doc.words)\n",
    "        train_vecs_dm.append(vec)\n",
    "    test_vecs_dm = []\n",
    "    for doc in test_tagged:\n",
    "        vec = d2v_model_dm.infer_vector(doc.words)\n",
    "        test_vecs_dm.append(vec)\n",
    "    return train_vecs_dm,test_vecs_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_baseline_model(optimizer='SGD',learn_rate=0.01, momentum=0):\n",
    "    '''\n",
    "    this function builds a default nn model that we will later fine tune\n",
    "    '''\n",
    "    batch_size = 100\n",
    "    epochs = 200\n",
    "    lr = 0.01\n",
    "    momentum = 0.2\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=input_dim, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256,activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='sigmoid'))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    #opt=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt =optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#model = nn_baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
